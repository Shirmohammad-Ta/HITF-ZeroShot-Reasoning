# HITF-ZeroShot-Reasoning

**Hybrid Instruction Tuning Framework (HITF)** is a lightweight yet powerful method to enhance the zero-shot reasoning capability of large language models (LLMs).  
It dynamically combines human-annotated and self-generated examples and leverages marginalization over intermediate responses to improve accuracy, logical coherence, and generalization.

## ğŸ”¬ Paper Reference
**Title:** Enhancing Zero-Shot Reasoning in Language Models via Hybrid Instruction Marginalization  
**Author:** Shirmohammad Tavangari  


## ğŸ§  Key Features
- Dynamic data selection (human vs self-generated)
- Intermediate response marginalization
- Zero-shot and few-shot support
- Compatible with GPT, T5, LLaMA

## ğŸš€ Quickstart
```bash
git clone https://github.com/your-username/HITF-ZeroShot-Reasoning.git
cd HITF-ZeroShot-Reasoning
pip install -r requirements.txt
```

## ğŸ—‚ï¸ Folder Structure
```
hitf/
  â”œâ”€â”€ model_selector.py
  â”œâ”€â”€ prompt_constructor.py
  â”œâ”€â”€ marginalizer.py
  â””â”€â”€ runner.py
data/
  â”œâ”€â”€ human_examples.json
  â””â”€â”€ self_generated.json
```

## ğŸ§ª Datasets
- SuperGLUE
- MMLU
- GSM8K
- FermiQA

## ğŸ“œ License
[MIT License](./LICENSE)

---

Made by [Shirmohammad Tavangari](mailto:s.tavangari@alumni.ubc.ca)
